import torch
import torch.nn as nn
import torch.nn.functional as F  # ã€æ–°å¢ã€‘éœ€è¦ç”¨åˆ° F


class BayesianSynergy(nn.Module):
    def __init__(self, dim=512):
        super().__init__()
        # è½»é‡çº§é—¨æ§ç½‘ç»œ
        self.gating = nn.Sequential(
            nn.Linear(1, 16),
            nn.ReLU(),
            nn.Linear(16, 3)
        )

        # ã€æ ¸å¿ƒä¿®å¤ã€‘æ’ç­‰åˆå§‹åŒ– (Identity Initialization)
        last_layer = self.gating[-1]
        nn.init.zeros_(last_layer.weight)

        # è®¾ç½® bias: [High, Low, Low] -> Softmax -> [1.0, 0.0, 0.0]
        custom_bias = torch.tensor([5.0, -5.0, -5.0])
        last_layer.bias.data.copy_(custom_bias)

        print(">>> [Synergy] Initialized with Identity Mapping (Trust Anchor 100%)")

    def forward(self, p_anchor, p_mamba, p_uot, confidence):
        """
        confidence: æ¥è‡ª UOT çš„ total_mass [B, 1, 1]
        """

        # ğŸš€ã€æ–°å¢å¿…æ€æŠ€ã€‘å¼ºåˆ¶å½’ä¸€åŒ– (LayerNorm)
        # è§£å†³ "Scale Mismatch" é—®é¢˜ï¼Œç¡®ä¿ Mamba çš„å¾®å¼±ä¿¡å·èƒ½è¢«åŒç­‰å¯¹å¾…
        # æ³¨æ„ï¼šæˆ‘ä»¬å¯¹æœ€åä¸€ä¸ªç»´åº¦ (dim=512) åšå½’ä¸€åŒ–
        p_anchor = F.layer_norm(p_anchor, p_anchor.shape[-1:])
        p_mamba = F.layer_norm(p_mamba, p_mamba.shape[-1:])
        p_uot = F.layer_norm(p_uot, p_uot.shape[-1:])

        # è®¡ç®— Logits
        logits = self.gating(confidence.squeeze(-1))  # [B, 3]

        # æ‰‹åŠ¨åš Softmax
        weights = torch.softmax(logits, dim=-1)

        w1, w2, w3 = weights[:, 0:1], weights[:, 1:2], weights[:, 2:3]

        # åŠ¨æ€åŠ æƒèåˆ
        p_next = w1.unsqueeze(-1) * p_anchor + \
                 w2.unsqueeze(-1) * p_mamba + \
                 w3.unsqueeze(-1) * p_uot

        return p_next