import math
import os
import torch
import torch.nn.functional as F
from torch import nn

from lib.models.layers.head import build_box_head
from lib.models.ostrack.hivit import hivit_base
from lib.models.ostrack.mamba_predictor import MambaPredictor
from lib.models.ostrack.uot_observer import UOTObserver
from lib.models.ostrack.synergy_update import BayesianSynergy
from lib.utils.box_ops import box_xyxy_to_cxcywh


class ProTeusH(nn.Module):
    def __init__(self, transformer, box_head, head_type="CENTER"):
        super().__init__()
        self.backbone = transformer
        self.box_head = box_head
        self.head_type = head_type

        # Phase 3 ç»„ä»¶
        self.predictor = MambaPredictor(dim=512)
        self.observer = UOTObserver(dim=512)
        self.synergy = BayesianSynergy(dim=512)

        # é›¶åˆå§‹åŒ–é˜€é—¨ (å®Œç¾ç»§æ‰¿ Phase 1 æ€§èƒ½çš„å…³é”®)
        self.fusion_alpha = nn.Parameter(torch.tensor(0.0))

        if head_type == "CORNER" or head_type == "CENTER":
            self.feat_sz_s = int(box_head.feat_sz)
            self.feat_len_s = int(box_head.feat_sz ** 2)

    def forward(self, template: torch.Tensor,
                search: torch.Tensor,
                prompt_history=None,
                **kwargs):

        B = template.shape[0]

        # 1. Anchor (Detached! æå…¶é‡è¦ï¼Œä¸è¦è®©æ¢¯åº¦å›ä¼ ç»™ backbone)
        with torch.no_grad():
            z_patch, _ = self.backbone.patch_embed(template)
            p_anchor = torch.mean(z_patch.reshape(B, -1, 512), dim=1, keepdim=True)
            p_anchor = p_anchor.detach()  # ğŸ”’ é”æ­» Anchor

        # 2. Mamba Prediction
        # åœ¨ ProTeusH.forward ä¸­ä¿®æ”¹è®­ç»ƒåˆ†æ”¯é€»è¾‘
        if prompt_history is None:
            # æ¨¡æ‹Ÿè®­ç»ƒé˜¶æ®µ
            prompt_history = p_anchor.repeat(1, 16, 1)

            if self.training:
                # ğŸš€ æ ¹æœ¬æ€§æ”¹è¿›ï¼šè®­ç»ƒæ—¶å¼•å…¥ 10% çš„æ—¶åºæ‰°åŠ¨å™ªå£°
                # å¼ºè¿« Backbone å­¦ä¼šçº æ­£é‚£äº›â€œç¨å¾®æœ‰ç‚¹åâ€çš„æ—¶åºç‰¹å¾ï¼Œè€Œä¸æ˜¯åªä¾èµ–å®Œç¾çš„ anchor
                noise = torch.randn_like(prompt_history) * 0.02
                prompt_history = prompt_history + noise

        p_prior = self.predictor(prompt_history).unsqueeze(1)

        # 3. Backbone Inference
        if template.shape[3] != search.shape[3]:
            padding_width = search.shape[3] - template.shape[3]
            template_padded = F.pad(template, (0, padding_width, 0, 0))
        else:
            template_padded = template
        x_in = torch.cat([template_padded, search], dim=2)

        # Backbone æ­£å¸¸å‰å‘ä¼ æ’­ (å…è®¸æ¢¯åº¦å›ä¼ )
        results = self.backbone(x_in)
        f3 = results[-1]
        visual_feats = f3.flatten(2).transpose(1, 2)

        # 4. UOT + Synergy
        p_obs, confidence = self.observer(p_prior, visual_feats)
        p_next = self.synergy(p_anchor, p_prior, p_obs, confidence)

        # 5. æ ¹æœ¬æ€§èåˆé‡æ„ï¼šUncertainty-Weighted Fusion
        alpha = torch.tanh(self.fusion_alpha)

        # ğŸš€ å…³é”®ï¼šåˆ©ç”¨ Synergy è®¡ç®—å‡ºçš„ confidence (æœ€ä¼˜ä¼ è¾“ä»£ä»·å¯¼å‡ºçš„ç½®ä¿¡åº¦)
        # å½“è§‚æµ‹ä¸é¢„æµ‹å†²çªå¾ˆå¤§æ—¶ï¼Œconfidence è¶‹äº 0ï¼Œè‡ªåŠ¨å…³é—­æ—¶åºåˆ†æ”¯å¯¹è§†è§‰ç‰¹å¾çš„å½±å“
        dynamic_alpha = alpha * torch.sigmoid(confidence)

        feat_scale = visual_feats.abs().mean().detach()
        p_next_scaled = F.normalize(p_next, dim=-1) * feat_scale

        # ä½¿ç”¨ dynamic_alpha è¿›è¡Œæ®‹å·®èåˆ
        refined_feats = visual_feats + dynamic_alpha * p_next_scaled

        out = self.forward_head(refined_feats)

        # Return history for next frame
        out['p_next'] = p_next
        out['p_anchor'] = p_anchor
        out['p_obs'] = p_obs  # <--- å…³é”®ä¿®å¤ï¼šå¿…é¡»ä¼ å‡ºè¿™ä¸ªè§‚æµ‹å€¼
        return out

    def forward_head(self, cat_feature):
        enc_opt = cat_feature[:, -self.feat_len_s:]
        opt = (enc_opt.unsqueeze(-1)).permute((0, 3, 2, 1)).contiguous()
        bs, Nq, C, HW = opt.size()
        opt_feat = opt.view(-1, C, self.feat_sz_s, self.feat_sz_s)

        if self.head_type == "CENTER":
            score_map_ctr, bbox, size_map, offset_map = self.box_head(opt_feat)
            return {'pred_boxes': bbox.view(bs, Nq, 4), 'score_map': score_map_ctr,
                    'size_map': size_map, 'offset_map': offset_map}
        else:
            raise NotImplementedError


def build_ostrack(cfg, training=True):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    pretrained_path = os.path.join(current_dir, '../../../pretrained_models')

    backbone = hivit_base()
    box_head = build_box_head(cfg, 512)
    model = ProTeusH(backbone, box_head, head_type=cfg.MODEL.HEAD.TYPE)

    if cfg.MODEL.PRETRAIN_FILE and training:
        ckpt_path = cfg.MODEL.PRETRAIN_FILE
        print(f">>> [Phase 3] Loading weights from: {ckpt_path}")

        # æ·»åŠ  weights_only=False
        checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)
        state_dict = checkpoint['net'] if 'net' in checkpoint else checkpoint

        model_dict = model.state_dict()
        new_dict = {}
        load_count = 0

        for k, v in state_dict.items():
            k_clean = k.replace('module.', '')
            if k_clean in model_dict:
                if v.shape == model_dict[k_clean].shape:
                    new_dict[k_clean] = v
                    load_count += 1

        if load_count == 0:
            raise ValueError("!!! No weights loaded! Check your checkpoint path or keys!")

        msg = model.load_state_dict(new_dict, strict=False)
        print(f">>> [Phase 3] Successfully loaded {load_count} keys.")

        # ç¡®è®¤ Box Head æ˜¯å¦åŠ è½½
        head_loaded = any("box_head" in k for k in new_dict.keys())
        if not head_loaded:
            raise ValueError("!!! Box Head weights NOT detected! Training will FAIL.")
        print(">>> [Phase 3] Box Head weights LOADED.")

    if training:
        mamba_path = os.path.join(pretrained_path, "mamba_phase2.pth")
        if os.path.exists(mamba_path):
            model.predictor.load_state_dict(torch.load(mamba_path, map_location='cpu', weights_only=False))
            print("[Phase 3] Loaded Mamba Pre-trained Weights.")
            # â„ï¸â„ï¸â„ï¸ ã€å¿…é¡»æ–°å¢ã€‘å†»ç»“ Mambaï¼Œè´¯å½» Anchor ç­–ç•¥ â„ï¸â„ï¸â„ï¸
            # åªæœ‰åŠ ä¸Šè¿™å°±è¯ï¼ŒBackbone æ‰ä¼šä¹–ä¹–å»é€‚åº” Mambaï¼Œè€Œä¸æ˜¯ä¸¤ä¸ªä¸€èµ·ä¹±è·‘
            for p in model.predictor.parameters():
                p.requires_grad = False
            print(">>> [Phase 3 Strategy] Mamba Predictor is FROZEN (Acting as Anchor).")
            # â„ï¸â„ï¸â„ï¸ ç»“æŸ â„ï¸â„ï¸â„ï¸
        else:
            print("[Warning] Mamba weights not found! Using Random Init.")

        # -------------------------------------------------------------
        # ğŸš€ã€SOTA å¿…åŠ ã€‘å¼ºåˆ¶è§£å†» Backbone
        # -------------------------------------------------------------
        # åŸå› ï¼šå¦‚æœä¸åŠ è¿™ä¸ªï¼ŒBackbone å¯èƒ½å› ä¸ºåŠ è½½äº†é¢„è®­ç»ƒæƒé‡è€Œä¿æŒ requires_grad=Falseã€‚
        # å½“ ltr_trainer åˆ›å»º optimizer æ—¶ï¼Œå®ƒä¼šæ£€æŸ¥ parameters()ã€‚
        # å¦‚æœæ­¤æ—¶æ˜¯ Falseï¼Œoptimizer å°±æ°¸è¿œä¸ä¼šåŒ…å«è¿™äº›å‚æ•°ï¼Œå¯¼è‡´ä½ ä»¥ä¸ºåœ¨å¾®è°ƒï¼Œå…¶å®æ²¡å¾®è°ƒã€‚
        for n, p in model.backbone.named_parameters():
            p.requires_grad = True
        print(">>> [Phase 3 SOTA Strategy] FORCED Backbone requires_grad = True. Ready for Full Finetuning.")
        # -------------------------------------------------------------

    return model