import math
import os
import torch
import torch.nn.functional as F
from torch import nn

from lib.models.layers.head import build_box_head
from lib.models.ostrack.hivit import hivit_base
from lib.models.ostrack.mamba_predictor import MambaPredictor
from lib.models.ostrack.uot_observer import UOTObserver
from lib.models.ostrack.synergy_update import BayesianSynergy
from lib.utils.box_ops import box_xyxy_to_cxcywh
# [åœ¨ ostrack.py å¼€å¤´å¼•å…¥]
import torch.nn.functional as F  # ç¡®ä¿å¼•å…¥ F

class ProTeusH(nn.Module):
    def __init__(self, transformer, box_head, head_type="CENTER"):
        super().__init__()
        self.backbone = transformer
        self.box_head = box_head
        self.head_type = head_type

        # Phase 3 æ ¸å¿ƒç»„ä»¶
        self.predictor = MambaPredictor(dim=512)
        self.observer = UOTObserver(dim=512)
        self.synergy = BayesianSynergy(dim=512)

        # ðŸ”´ [åˆ é™¤æ—§ä»£ç ]
        # self.spatial_align = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)

        # ðŸŸ¢ [æ–°å¢žä»£ç ] ä½¿ç”¨é—¨æŽ§é€šé“è°ƒåˆ¶ (FiLM æœºåˆ¶çš„å˜ä½“)
        # å°† p_next (B, 1, 512) æ˜ å°„ä¸ºç¼©æ”¾å› å­ (Scale) å’Œ åç½® (Shift)
        self.fusion_map = nn.Linear(512, 512 * 2)
        # åˆå§‹åŒ–ä¸º "æ— æ“ä½œ" çŠ¶æ€ (Scale=1, Shift=0)
        nn.init.constant_(self.fusion_map.weight, 0)
        nn.init.constant_(self.fusion_map.bias, 0)

        # è¿™ä¸€è¡Œå¯ä»¥ä¿ç•™ï¼Œç”¨äºŽæŽ§åˆ¶æ•´ä½“åŠ›åº¦
        self.fusion_alpha = nn.Parameter(torch.tensor(0.0))

        if head_type == "CORNER" or head_type == "CENTER":
            self.feat_sz_s = int(box_head.feat_sz)
            self.feat_len_s = int(box_head.feat_sz ** 2)

    # [åœ¨ ProTeusH ç±» forward å‡½æ•°ä¸­ä¿®æ”¹]
    def forward(self, template, search, ce_template_mask=None, ce_keep_rate=None, prompt_history=None, **kwargs):
        B = template.shape[0]

        # 1. Anchor é”æ­»
        with torch.no_grad():
            z_patch, _ = self.backbone.patch_embed(template)
            p_anchor = torch.mean(z_patch.reshape(B, -1, 512), dim=1, keepdim=True).detach()

        # 2. å¯¹é½è®­ç»ƒ/æŽ¨ç†åˆ†å¸ƒ & ðŸ”´ [å…³é”®ä¿®å¤ï¼šè¾“å…¥å½’ä¸€åŒ–]
        if prompt_history is None:
            # è®­ç»ƒæ—¶çš„â€œå‡æ—¶åºâ€å¢žå¼ºï¼šå¢žåŠ æ›´å¤§çš„å™ªå£°æ¥æ¨¡æ‹Ÿè¿åŠ¨ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆé™æ€
            prompt_history = p_anchor.repeat(1, 16, 1)
            if self.training:
                # å¢žå¤§å™ªå£°å¹…åº¦ (0.02 -> 0.05)ï¼Œæ¨¡æ‹Ÿè¿™ä¸€ç§’å†…çš„å˜åŒ–
                prompt_history = prompt_history + torch.randn_like(prompt_history) * 0.05

        # ðŸŸ¢ [å…³é”®ä¿®å¤] Mamba é¢„è®­ç»ƒæ—¶ç”¨äº† F.normalizeï¼Œè¿™é‡Œå¿…é¡»åŠ ä¸Šï¼
        # å¦åˆ™è¾“å…¥çš„æ¨¡é•¿å·®å¼‚ä¼šå¯¼è‡´ Mamba è¾“å‡ºä¹±ç 
        prompt_history_norm = F.normalize(prompt_history, p=2, dim=-1)
        p_prior = self.predictor(prompt_history_norm).unsqueeze(1)

        # 3. Backbone Inference (ä¿æŒä¸å˜)
        if template.shape[3] != search.shape[3]:
            padding_width = search.shape[3] - template.shape[3]
            template_padded = F.pad(template, (0, padding_width, 0, 0))
        else:
            template_padded = template
        x_in = torch.cat([template_padded, search], dim=2)

        results = self.backbone(x_in)
        f3 = results[-1]
        f3_flat = f3.flatten(2).transpose(1, 2)
        visual_feats = f3_flat[:, -self.feat_len_s:]

        # 4. UOT + Synergy (ä¿æŒä¸å˜)
        p_obs, confidence = self.observer(p_prior, visual_feats)
        p_next = self.synergy(p_anchor, p_prior, p_obs, confidence)

        # ============================================================
        # ðŸŸ¢ [å…³é”®ä¿®å¤] æ›¿æ¢é”™è¯¯çš„ Attentionï¼Œæ”¹ç”¨é€šé“è°ƒåˆ¶
        # ============================================================

        # p_next: [B, 1, 512]
        # ç”Ÿæˆè°ƒåˆ¶å‚æ•°: [B, 1, 1024] -> split -> scale, shift: [B, 1, 512]
        style = self.fusion_map(p_next)
        scale, shift = style.chunk(2, dim=-1)

        # é—¨æŽ§ç³»æ•°
        alpha = torch.tanh(self.fusion_alpha)

        # è°ƒåˆ¶å…¬å¼: Visual * (1 + Scale) + Shift
        # è¿™æ · p_next å¯ä»¥æŒ‰é€šé“å¢žå¼ºæˆ–æŠ‘åˆ¶ Visual Feature
        modulated_feats = visual_feats * (1.0 + alpha * torch.sigmoid(scale)) + alpha * shift

        # æ®‹å·®è¿žæŽ¥ (å¯é€‰ï¼Œä½†æŽ¨èä¿ç•™åŽŸå§‹ç‰¹å¾åº•åº§)
        refined_feats = visual_feats + modulated_feats

        # ============================================================

        out = self.forward_head(refined_feats)
        # ðŸ”´ [å…³é”®] å¿…é¡»æŠŠ p_next ç­‰ä¼ å‡ºåŽ»ï¼Œå¦‚æžœä½ è¿˜è¦ç”¨ Loss (è™½ç„¶å»ºè®®åŽ»æŽ‰ REG Loss)
        out.update({'p_next': p_next, 'p_anchor': p_anchor, 'p_obs': p_obs})
        return out

    def forward_head(self, cat_feature):
        # è¿™é‡Œçš„ cat_feature å·²ç»æ˜¯åˆ‡ç‰‡åŽçš„ search tokens
        # å³ä½¿è¿™é‡Œå†æ¬¡åˆ‡ç‰‡ä¹Ÿæ²¡å…³ç³»ï¼Œå› ä¸ºé•¿åº¦æ­£å¥½æ˜¯ feat_len_s
        enc_opt = cat_feature[:, -self.feat_len_s:]

        # [B, N, C] -> [B, C, N] -> [B, C, H, W]
        opt = (enc_opt.unsqueeze(-1)).permute((0, 3, 2, 1)).contiguous()
        bs, Nq, C, HW = opt.size()
        opt_feat = opt.view(-1, C, self.feat_sz_s, self.feat_sz_s)

        if self.head_type == "CENTER":
            score_map_ctr, bbox, size_map, offset_map = self.box_head(opt_feat)
            return {'pred_boxes': bbox.view(bs, Nq, 4), 'score_map': score_map_ctr,
                    'size_map': size_map, 'offset_map': offset_map}
        else:
            raise NotImplementedError


def build_ostrack(cfg, training=True):
    current_dir = os.path.dirname(os.path.abspath(__file__))
    pretrained_path = os.path.join(current_dir, '../../../pretrained_models')

    backbone = hivit_base()
    box_head = build_box_head(cfg, 512)
    model = ProTeusH(backbone, box_head, head_type=cfg.MODEL.HEAD.TYPE)

    if cfg.MODEL.PRETRAIN_FILE and training:
        ckpt_path = cfg.MODEL.PRETRAIN_FILE
        print(f">>> [Phase 3] Loading weights from: {ckpt_path}")

        # æ·»åŠ  weights_only=False
        checkpoint = torch.load(ckpt_path, map_location='cpu', weights_only=False)
        state_dict = checkpoint['net'] if 'net' in checkpoint else checkpoint

        model_dict = model.state_dict()
        new_dict = {}
        load_count = 0

        for k, v in state_dict.items():
            k_clean = k.replace('module.', '')
            if k_clean in model_dict:
                if v.shape == model_dict[k_clean].shape:
                    new_dict[k_clean] = v
                    load_count += 1

        if load_count == 0:
            raise ValueError("!!! No weights loaded! Check your checkpoint path or keys!")

        msg = model.load_state_dict(new_dict, strict=False)
        print(f">>> [Phase 3] Successfully loaded {load_count} keys.")

        # ç¡®è®¤ Box Head æ˜¯å¦åŠ è½½
        head_loaded = any("box_head" in k for k in new_dict.keys())
        if not head_loaded:
            raise ValueError("!!! Box Head weights NOT detected! Training will FAIL.")
        print(">>> [Phase 3] Box Head weights LOADED.")

    # [åœ¨ build_ostrack å‡½æ•°ä¸­ä¿®æ”¹]
    if training:
        mamba_path = os.path.join(pretrained_path, "mamba_phase2.pth")
        if os.path.exists(mamba_path):
            model.predictor.load_state_dict(torch.load(mamba_path, map_location='cpu', weights_only=False))
            print("[Phase 3] Loaded Mamba Pre-trained Weights.")

            # ðŸ”´ [åˆ é™¤] ä¸è¦å†»ç»“ï¼
            # for p in model.predictor.parameters():
            #     p.requires_grad = False
            # print(">>> [Phase 3 Strategy] Mamba Predictor is FROZEN...")

            # ðŸŸ¢ [æ–°å¢ž] ç¡®ä¿è§£å†»ï¼Œå…è®¸ Mamba é€‚åº”æ–°çš„ Backbone ç‰¹å¾åˆ†å¸ƒ
            for p in model.predictor.parameters():
                p.requires_grad = True
            print(">>> [Phase 3 Strategy] Mamba Predictor UNLOCKED for Co-adaptation.")

        else:
            print("[Warning] Mamba weights not found! Using Random Init.")

        # -------------------------------------------------------------
        # ðŸš€ã€SOTA å¿…åŠ ã€‘å¼ºåˆ¶è§£å†» Backbone
        # -------------------------------------------------------------
        # ç¡®ä¿ optimizer èƒ½æ³¨å†Œåˆ°å‚æ•°
        for n, p in model.backbone.named_parameters():
            p.requires_grad = True
        print(">>> [Phase 3 SOTA Strategy] FORCED Backbone requires_grad = True. Ready for Full Finetuning.")
        # -------------------------------------------------------------

    return model